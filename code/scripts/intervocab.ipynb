{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer\n",
    "gpt_model_path = \"/mnt/data/users/xuziyang/huggingface_model/gpt2-large\"\n",
    "bert_model_path =  \"/mnt/data/users/xuziyang/huggingface_model/bert-base-cased\"\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_path)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "# model = GPT2Model.from_pretrained(model_path)\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的模块适用于LLAMA模型\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "gpt_model_path = \"/mnt/data/users/xuziyang/huggingface_model/shares/llama2-convert-7b-hf\"\n",
    "bert_model_path =  \"/mnt/data/users/xuziyang/huggingface_model/bert-base-cased\"\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_path)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)\n",
    "# model = GPT2Model.from_pretrained(model_path)\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_vocabs = list(gpt_tokenizer.get_vocab().keys())\n",
    "bert_vocabs = list(bert_tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 去除bert中的非独立单词 去除gpt中的空白符号\n",
    "bert_vocabs = set(filter(lambda x:x[:2]!=\"##\",bert_vocabs))\n",
    "gpt_vocabs = {i.replace(\"Ġ\",'') for i in gpt_vocabs}\n",
    "# 获取common_vocab\n",
    "common_vocab = bert_vocabs.intersection(gpt_vocabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"London\" in gpt_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyone\n",
      "along\n",
      "one\n",
      "at\n",
      "although\n",
      "thus\n",
      "am\n",
      "that\n",
      "last\n",
      "ever\n",
      "eight\n",
      "somehow\n",
      "such\n",
      "well\n",
      "latter\n",
      "until\n",
      "since\n",
      "side\n",
      "everything\n",
      "yourself\n",
      "who\n",
      "from\n",
      "no\n",
      "has\n",
      "together\n",
      "our\n",
      "a\n",
      "itself\n",
      "what\n",
      "could\n",
      "or\n",
      "nobody\n",
      "regarding\n",
      "anywhere\n",
      "fifteen\n",
      "would\n",
      "might\n",
      "without\n",
      "every\n",
      "it\n",
      "yet\n",
      "many\n",
      "serious\n",
      "else\n",
      "nowhere\n",
      "make\n",
      "thereafter\n",
      "whom\n",
      "becoming\n",
      "top\n",
      "further\n",
      "sixty\n",
      "hence\n",
      "six\n",
      "back\n",
      "more\n",
      "only\n",
      "herself\n",
      "various\n",
      "perhaps\n",
      "towards\n",
      "behind\n",
      "we\n",
      "beyond\n",
      "throughout\n",
      "own\n",
      "made\n",
      "some\n",
      "will\n",
      "though\n",
      "themselves\n",
      "therefore\n",
      "next\n",
      "indeed\n",
      "again\n",
      "all\n",
      "four\n",
      "five\n",
      "either\n",
      "nor\n",
      "none\n",
      "another\n",
      "yours\n",
      "them\n",
      "into\n",
      "down\n",
      "under\n",
      "everywhere\n",
      "most\n",
      "keep\n",
      "others\n",
      "whatever\n",
      "alone\n",
      "per\n",
      "seem\n",
      "enough\n",
      "namely\n",
      "nothing\n",
      "show\n",
      "thence\n",
      "here\n",
      "my\n",
      "before\n",
      "still\n",
      "of\n",
      "full\n",
      "whoever\n",
      "off\n",
      "using\n",
      "give\n",
      "seemed\n",
      "after\n",
      "via\n",
      "how\n",
      "they\n",
      "fifty\n",
      "forty\n",
      "somewhere\n",
      "beside\n",
      "when\n",
      "thereby\n",
      "anyway\n",
      "which\n",
      "himself\n",
      "during\n",
      "he\n",
      "on\n",
      "cannot\n",
      "may\n",
      "nine\n",
      "never\n",
      "where\n",
      "an\n",
      "have\n",
      "been\n",
      "call\n",
      "often\n",
      "third\n",
      "former\n",
      "did\n",
      "against\n",
      "also\n",
      "just\n",
      "neither\n",
      "then\n",
      "mostly\n",
      "once\n",
      "their\n",
      "up\n",
      "whole\n",
      "less\n",
      "whereby\n",
      "used\n",
      "three\n",
      "but\n",
      "by\n",
      "me\n",
      "already\n",
      "go\n",
      "seeming\n",
      "much\n",
      "wherein\n",
      "in\n",
      "however\n",
      "him\n",
      "take\n",
      "doing\n",
      "first\n",
      "for\n",
      "someone\n",
      "see\n",
      "becomes\n",
      "move\n",
      "do\n",
      "unless\n",
      "i\n",
      "was\n",
      "sometimes\n",
      "across\n",
      "nevertheless\n",
      "while\n",
      "should\n",
      "your\n",
      "toward\n",
      "are\n",
      "now\n",
      "her\n",
      "become\n",
      "upon\n",
      "among\n",
      "she\n",
      "over\n",
      "ten\n",
      "twenty\n",
      "rather\n",
      "same\n",
      "below\n",
      "almost\n",
      "whereas\n",
      "several\n",
      "whether\n",
      "put\n",
      "really\n",
      "had\n",
      "you\n",
      "eleven\n",
      "myself\n",
      "onto\n",
      "and\n",
      "sometime\n",
      "wherever\n",
      "hundred\n",
      "these\n",
      "than\n",
      "if\n",
      "get\n",
      "to\n",
      "became\n",
      "except\n",
      "seems\n",
      "as\n",
      "any\n",
      "bottom\n",
      "were\n",
      "two\n",
      "done\n",
      "part\n",
      "hers\n",
      "not\n",
      "must\n",
      "anything\n",
      "between\n",
      "does\n",
      "its\n",
      "few\n",
      "something\n",
      "be\n",
      "elsewhere\n",
      "whose\n",
      "least\n",
      "other\n",
      "being\n",
      "always\n",
      "the\n",
      "please\n",
      "name\n",
      "is\n",
      "afterwards\n",
      "above\n",
      "twelve\n",
      "otherwise\n",
      "us\n",
      "ours\n",
      "say\n",
      "whenever\n",
      "within\n",
      "meanwhile\n",
      "both\n",
      "ca\n",
      "mine\n",
      "front\n",
      "everyone\n",
      "because\n",
      "can\n",
      "re\n",
      "each\n",
      "this\n",
      "empty\n",
      "even\n",
      "about\n",
      "ourselves\n",
      "too\n",
      "quite\n",
      "why\n",
      "amongst\n",
      "there\n",
      "formerly\n",
      "through\n",
      "very\n",
      "his\n",
      "around\n",
      "amount\n",
      "besides\n",
      "out\n",
      "due\n",
      "with\n",
      "those\n",
      "so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 240/18354 [00:01<01:16, 236.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: `\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 813/18354 [00:03<01:16, 230.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1732/18354 [00:07<01:09, 238.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 2481/18354 [00:10<01:07, 235.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ÷\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 2941/18354 [00:12<01:04, 237.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3209/18354 [00:13<01:03, 237.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3546/18354 [00:15<01:02, 236.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3693/18354 [00:15<01:02, 233.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im\n",
      "0 - i\n",
      "1 - m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 4695/18354 [00:20<00:58, 231.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wed\n",
      "0 - we\n",
      "1 - d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 5911/18354 [00:25<00:53, 230.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6055/18354 [00:26<00:53, 231.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6510/18354 [00:28<00:51, 228.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6893/18354 [00:29<00:49, 233.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gonna\n",
      "0 - gon\n",
      "1 - na\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 8021/18354 [00:34<00:44, 233.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "0 - i\n",
      "1 - d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 8899/18354 [00:38<00:40, 233.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: @\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10238/18354 [00:44<00:34, 232.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10785/18354 [00:46<00:32, 231.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ^\n",
      "PUNCT: ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10905/18354 [00:47<00:31, 234.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYM: $\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 10979/18354 [00:47<00:31, 235.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: neo\n",
      "PUNCT: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 12465/18354 [00:53<00:25, 233.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 12751/18354 [00:54<00:24, 233.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: ¥\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 13087/18354 [00:56<00:22, 233.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: »\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 15007/18354 [01:04<00:14, 233.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 15895/18354 [01:08<00:10, 230.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: !\n",
      "PUNCT: «\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 16015/18354 [01:08<00:09, 234.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: 00\n",
      "PUNCT: >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 16471/18354 [01:10<00:08, 230.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: /\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 16925/18354 [01:12<00:06, 231.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: kg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 16997/18354 [01:13<00:05, 234.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 17645/18354 [01:15<00:03, 236.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotta\n",
      "0 - got\n",
      "1 - ta\n",
      "PUNCT: iOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 17837/18354 [01:16<00:02, 235.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYM: #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 18053/18354 [01:17<00:01, 236.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT: _\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18354/18354 [01:18<00:00, 232.62it/s]\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_SYMBOLS = list(bert_tokenizer.special_tokens_map.values()) + list(gpt_tokenizer.special_tokens_map.values())\n",
    "SPECIAL_SYMBOLS = set(SPECIAL_SYMBOLS)\n",
    "\n",
    "# no special symbols in common_vocab\n",
    "for symbol in SPECIAL_SYMBOLS:\n",
    "    if symbol in common_vocab:\n",
    "        common_vocab.remove(symbol)\n",
    "\n",
    "# remove stop words\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "for stop_word in STOP_WORDS:\n",
    "    if stop_word in common_vocab:\n",
    "        print(stop_word)\n",
    "        common_vocab.remove(stop_word)\n",
    "\n",
    "common_vocab = list(common_vocab)\n",
    "\n",
    "# remove punctuation and symbols\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "manual_punctuation = ['(', ')', '.', ',']\n",
    "new_common_vocab = []\n",
    "for i in tqdm(range(len(common_vocab))):\n",
    "    word = common_vocab[i]\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "    if(len(doc) != 1):\n",
    "        print(word)\n",
    "        for idx, tok in enumerate(doc):\n",
    "            print(\"{} - {}\".format(idx, tok))\n",
    "    elif word in manual_punctuation:\n",
    "        pass\n",
    "    elif token.pos_ == \"PUNCT\":\n",
    "        print(\"PUNCT: {}\".format(word))\n",
    "    elif token.pos_ == \"SYM\":\n",
    "        print(\"SYM: {}\".format(word))\n",
    "    else:\n",
    "        new_common_vocab.append(word)\n",
    "    # print(\"{} - {}\".format(word, token.pos_))\n",
    "common_vocab = new_common_vocab\n",
    "\n",
    "# store common_vocab on file\n",
    "filename = \"/mnt/code/users/xuziyang/PromptBias/common_vocabs/common_vocab_cased_bert_llama.txt\"\n",
    "with open(filename, 'w') as f:\n",
    "    for item in sorted(common_vocab):\n",
    "        f.write(\"{}\\n\".format(item))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2a961f21e3910a1829c13a9982b953d485249e8a0f32b2fb196974498b9d8dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.17 ('xzy_BiasBench')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
